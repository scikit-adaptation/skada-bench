

<!DOCTYPE html>
<html class="writer-html5" lang="English" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>SKADA Benchmark &mdash; SKADA Benchmark</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=76184ce0"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="How to Contribute to SKADA-Bench" href="contribute.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
    <a href="#" class="icon icon-home"> SKADA Benchmark</a>
    

        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">SKADA Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="contribute.html">How to Contribute to SKADA-Bench</a></li>
<li class="toctree-l1"><a class="reference internal" href="results.html">Results</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">SKADA Benchmark</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">SKADA Benchmark</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="skada-benchmark">
<h1>SKADA Benchmark<a class="headerlink" href="#skada-benchmark" title="Link to this heading"></a></h1>
<a class="reference internal image-reference" href="_images/skada_logo_full.svg"><img alt="SKADA Logo" src="_images/skada_logo_full.svg" style="width: 600px;" />
</a>
<section id="contents">
<h2>Contents<a class="headerlink" href="#contents" title="Link to this heading"></a></h2>
<div class="toctree-wrapper compound">
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">SKADA Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="contribute.html">How to Contribute to SKADA-Bench</a></li>
<li class="toctree-l1"><a class="reference internal" href="results.html">Results</a></li>
</ul>
</div>
<section id="skada-bench-benchmarking-unsupervised-domain-adaptation-methods-with-realistic-validation">
<h3>SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with Realistic Validation<a class="headerlink" href="#skada-bench-benchmarking-unsupervised-domain-adaptation-methods-with-realistic-validation" title="Link to this heading"></a></h3>
<p>Welcome to the official implementation of <a class="reference external" href="https://arxiv.org/pdf/2407.11676">SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with Realistic Validation</a></p>
<p>To reproduce the results in this paper, you have three options:</p>
<ul class="simple">
<li><p><strong>Visualize stored results (Step 3):</strong> with results in <code class="docutils literal notranslate"><span class="pre">/visualize/cleaned_outputs/</span></code>.</p></li>
<li><p><strong>Run bench (Step 2):</strong> with config files in <code class="docutils literal notranslate"><span class="pre">/config/datasets</span></code>.</p></li>
<li><p><strong>Start from Scratch (Step 1):</strong> Generate new config files and proceed from there.</p></li>
</ul>
<p>We provided all the necessary files for running each step without the need for
running the previous one.</p>
<p>If you want to add new solvers, dataset or scorers just follow the instructions in the
<a class="reference internal" href="#CONTRIBUTE.md"><span class="xref myst">CONTRIBUTE.md</span></a> file.</p>
<blockquote>
<div><p><strong>Note:</strong> Current implementation reproduces results from the paper, with minor variations in some cases. Expect minor code adjustments in coming weeks to improve reproducibility. A revised paper version with near-exact reproducibility is also planned.</p>
</div></blockquote>
<section id="requirements">
<h4>Requirements<a class="headerlink" href="#requirements" title="Link to this heading"></a></h4>
<p><strong>Note: This project requires Python 3.10.</strong></p>
<p>To install the necessary requirements to run a benchmark, use the following commands:</p>
<ol class="arabic simple">
<li><p>Ensure you have Python 3.10 installed. You can check your Python version with:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>--version
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Install the benchopt library:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">benchopt</span><span class="o">==</span><span class="m">1</span>.6.0
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Install the desired datasets and solvers using <code class="docutils literal notranslate"><span class="pre">benchopt</span></code>. Specify the dataset and solver you want to use (e.g., <code class="docutils literal notranslate"><span class="pre">simulated</span></code> and <code class="docutils literal notranslate"><span class="pre">bci</span></code> solver):</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>benchopt<span class="w"> </span>install<span class="w"> </span>.<span class="w">  </span><span class="o">[</span>--download<span class="o">]</span>
</pre></div>
</div>
<p><strong>Note:</strong> The <code class="docutils literal notranslate"><span class="pre">--download</span></code> flag is optional but highly recommended. It pre-downloads the datasets, which is particularly useful in the following scenarios:</p>
<ul class="simple">
<li><p>When working on large clusters where internet access might be limited on computing nodes.</p></li>
<li><p>To avoid multiple processes attempting to download data simultaneously.</p></li>
<li><p>To ensure data is properly loaded when installing the benchmark.</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p>[NOT MANDATORY] Install the preprocessing - visualising - all requirements:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>preprocessing/requirements_preprocess.txt<span class="w"> </span><span class="c1"># Install preprocessing dependencies</span>
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>visualize/requirements_plot.txt<span class="w"> </span><span class="c1"># Install plotting dependencies</span>
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements_all.txt<span class="w"> </span><span class="c1"># Install all dependencies</span>
</pre></div>
</div>
</section>
<section id="running-the-benchmark">
<h4>Running the Benchmark<a class="headerlink" href="#running-the-benchmark" title="Link to this heading"></a></h4>
<section id="step-1-shallow-methods-only-finding-the-best-base-estimators-for-each-dataset">
<h5>Step 1 (shallow methods only): Finding the Best Base Estimators for Each Dataset<a class="headerlink" href="#step-1-shallow-methods-only-finding-the-best-base-estimators-for-each-dataset" title="Link to this heading"></a></h5>
<section id="generate-the-configuration-file">
<h6>1.1 Generate the Configuration File<a class="headerlink" href="#generate-the-configuration-file" title="Link to this heading"></a></h6>
<p>Generate the config file for selecting base estimator on source:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>benchmark_utils/generate_config/generate_base_estim_config.py
</pre></div>
</div>
<p>This generates <code class="docutils literal notranslate"><span class="pre">config/find_best_base_estimators_per_dataset.yml</span></code>.</p>
</section>
<section id="run-base-estimator-selection-bench">
<h6>1.2 Run base estimator selection bench<a class="headerlink" href="#run-base-estimator-selection-bench" title="Link to this heading"></a></h6>
<p>Run base estimator experiments and store the results:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>benchopt<span class="w"> </span>run<span class="w"> </span>--config<span class="w"> </span>config/find_best_base_estimators_per_dataset.yml<span class="w"> </span>--output<span class="w"> </span>base_estimators/results_base_estimators<span class="w"> </span>--no-plot<span class="w"> </span>--no-html
</pre></div>
</div>
<p>This generates <code class="docutils literal notranslate"><span class="pre">outputs/base_estimators/results_base_estimators</span></code>.</p>
</section>
<section id="extract-the-results">
<h6>1.3 Extract the results<a class="headerlink" href="#extract-the-results" title="Link to this heading"></a></h6>
<p>Extract the results and store them in a CSV file <code class="docutils literal notranslate"><span class="pre">results_base_estimators/</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>visualize/convert_benchopt_output_to_readable_csv.py<span class="w"> </span>--domain<span class="w"> </span><span class="nb">source</span><span class="w"> </span>--directory<span class="w"> </span>outputs/base_estimators<span class="w"> </span>--output<span class="w"> </span>results_base_estimators<span class="w"> </span>--file_name<span class="w"> </span>results_base_estim_experiments
</pre></div>
</div>
<p>This generates <code class="docutils literal notranslate"><span class="pre">results_base_estimators/results_base_estim_experiments.csv</span></code>.</p>
</section>
<section id="find-the-best-base-estimators">
<h6>1.4 Find the best base estimators<a class="headerlink" href="#find-the-best-base-estimators" title="Link to this heading"></a></h6>
<p>Find the best base estimator per dataset and store them in <code class="docutils literal notranslate"><span class="pre">config/best_base_estimators.yml</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>benchmark_utils/extract_best_base_estim.py
</pre></div>
</div>
<p>This generates <code class="docutils literal notranslate"><span class="pre">config/best_base_estimators.yml</span></code>.</p>
</section>
<section id="generate-configurations-with-best-base-estimators">
<h6>1.5 Generate Configurations with Best Base Estimators<a class="headerlink" href="#generate-configurations-with-best-base-estimators" title="Link to this heading"></a></h6>
<p>Update the config file per dataset with the best base estimator:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>benchmark_utils/generate_config/generate_config_per_dataset.py
</pre></div>
</div>
<p>This generates a config file for each dataset in <code class="docutils literal notranslate"><span class="pre">config/datasets/</span></code>.</p>
</section>
</section>
<section id="step-2-launch-benchmark-for-each-dataset">
<h5>Step 2: Launch benchmark for each Dataset<a class="headerlink" href="#step-2-launch-benchmark-for-each-dataset" title="Link to this heading"></a></h5>
<p>To launch the benchmark for each dataset, run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>benchopt<span class="w"> </span>run<span class="w"> </span>--config<span class="w"> </span>dataset.yml<span class="w"> </span>--timeout<span class="w"> </span>3h<span class="w"> </span>--output<span class="w"> </span>output_directory/output_dataset<span class="w"> </span>--no-plot<span class="w"> </span>--no-html
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dataset.yml</span></code>: Config file of the specified dataset.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_directory</span></code>: Name of the output directory (<code class="docutils literal notranslate"><span class="pre">real_datasets</span></code> or <code class="docutils literal notranslate"><span class="pre">simulated_datasets</span></code> depending on your data)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_dataset</span></code>: Name of the output result parquet/csv.</p></li>
</ul>
<section id="example-simulated-dataset">
<h6>Example: Simulated Dataset<a class="headerlink" href="#example-simulated-dataset" title="Link to this heading"></a></h6>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>benchopt<span class="w"> </span>run<span class="w"> </span>--config<span class="w"> </span>config/datasets/Simulated.yml<span class="w"> </span>--timeout<span class="w"> </span>3h<span class="w"> </span>--output<span class="w"> </span>simulated_datasets/output_simulated<span class="w"> </span>--no-plot<span class="w"> </span>--no-html
</pre></div>
</div>
<blockquote>
<div><p><strong>Note:</strong> In the paper results, the timeout was set to 3 hours for shallow methods and 24 hours for deep methods.
The <code class="docutils literal notranslate"><span class="pre">benchopt</span></code> framework supports running benchmarks in parallel on a SLURM cluster. For more details, refer to the <a class="reference external" href="https://benchopt.github.io/user_guide/advanced.html">Benchopt user guide</a>.</p>
</div></blockquote>
</section>
</section>
</section>
<section id="step-3-displaying-results">
<h4>Step 3: Displaying Results<a class="headerlink" href="#step-3-displaying-results" title="Link to this heading"></a></h4>
<p>Convert the <code class="docutils literal notranslate"><span class="pre">benchopt</span></code> output into a CSV format:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>visualize/convert_benchopt_output_to_readable_csv.py<span class="w"> </span>--directory<span class="w"> </span>outputs/simulated_datasets<span class="w"> </span>--domain<span class="w"> </span>target<span class="w"> </span>--file_name<span class="w"> </span>output_readable_dataset
</pre></div>
</div>
<p>This generates <code class="docutils literal notranslate"><span class="pre">visualize/cleaned_outputs/output_readable_dataset.csv</span></code>. This csv file can then be used by anyone to plot the benchmarking results.</p>
<section id="visualization-commands">
<h5>Visualization Commands<a class="headerlink" href="#visualization-commands" title="Link to this heading"></a></h5>
<p>In the <code class="docutils literal notranslate"><span class="pre">visualize</span></code> folder, run the following commands to generate various results and plots:</p>
<section id="shallow-methods">
<h6>Shallow Methods<a class="headerlink" href="#shallow-methods" title="Link to this heading"></a></h6>
<ul>
<li><p><strong>Main Result Table (Shallow):</strong>|</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>plot_results_all_datasets.py<span class="w"> </span>--csv-file<span class="w"> </span>cleaned_outputs/results_real_datasets_experiments.csv<span class="w"> </span>--csv-file-simulated<span class="w"> </span>cleaned_outputs/results_simulated_datasets_experiments.csv
</pre></div>
</div>
</li>
<li><p><strong>Individual Tables per Dataset:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>plot_results_per_dataset.py<span class="w"> </span>--csv-file<span class="w"> </span>cleaned_outputs/results_real_datasets_experiments.csv<span class="w"> </span>--dataset<span class="w"> </span>BCI
</pre></div>
</div>
</li>
<li><p><strong>Cross-val Score vs. Accuracy for Different Scorers:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>plot_inner_score_vs_acc.py<span class="w"> </span>--csv-file<span class="w"> </span>cleaned_outputs/results_real_datasets_experiments.csv
</pre></div>
</div>
</li>
<li><p><strong>Accuracy of DA Methods using Unsupervised Scorers vs. Supervised Scorers:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>plot_supervised_vs_unsupervised.py<span class="w"> </span>--csv-file<span class="w"> </span>cleaned_outputs/results_real_datasets_experiments.csv
</pre></div>
</div>
</li>
<li><p><strong>Change in Accuracy of DA Methods with Best Unsupervised Scorer vs. Supervised Scorer:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>plot_boxplot.py<span class="w">  </span>--csv-file<span class="w"> </span>cleaned_outputs/results_real_datasets_experiments.csv
</pre></div>
</div>
</li>
</ul>
</section>
<section id="deep-methods">
<h6>Deep Methods<a class="headerlink" href="#deep-methods" title="Link to this heading"></a></h6>
<ul>
<li><p><strong>Main Result Table (Deep):</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>plot_results_all_datasets_deep.py<span class="w"> </span>--csv-file<span class="w"> </span>cleaned_outputs/results_deep_datasets_experiments.csv<span class="w"> </span>--scorer-selection<span class="w"> </span>unsupervised
</pre></div>
</div>
</li>
</ul>
</section>
<section id="both-shallow-and-deep-methods">
<h6>Both Shallow and Deep Methods<a class="headerlink" href="#both-shallow-and-deep-methods" title="Link to this heading"></a></h6>
<ul>
<li><p><strong>Mean Computing Time for Training and Testing Each Method:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>visualize/get_computational_time.py<span class="w"> </span>--directory<span class="w"> </span>outputs
</pre></div>
</div>
</li>
</ul>
<p>All the generated tables and plots can be found in the <code class="docutils literal notranslate"><span class="pre">visualize</span></code> folder.</p>
<blockquote>
<div><p><strong>Note:</strong> For the <code class="docutils literal notranslate"><span class="pre">get_computational_time</span></code> script, you need to give directly benchopt outputs which are not provided due to size limits (all other results are provided).</p>
</div></blockquote>
<p>Happy benchmarking!</p>
</section>
</section>
</section>
</section>
</section>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Link to this heading"></a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="contribute.html" class="btn btn-neutral float-right" title="How to Contribute to SKADA-Bench" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, The SKADA team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<div class="rst-versions" data-toggle="rst-versions" role="note"
aria-label="versions">
  <!--  add shift_up to the class for force viewing ,
  data-toggle="rst-current-version" -->
    <span class="rst-current-version"  style="margin-bottom:1mm;">
      <span class="fa fa-book"> SKADA-Bench</span>
      <hr  style="margin-bottom:1.5mm;margin-top:5mm;">
     <!--  versions
      <span class="fa fa-caret-down"></span>-->
      <span class="rst-current-version" style="display: inline-block;padding:
      0px;color:#fcfcfcab;float:left;font-size: 100%;">
        <a href="../../all_versions.html" 
        style="padding: 3px;color:#fcfcfc;font-size: 100%;">View all versions</a>

      </span>

     
    </span>
  
  </div>


</body>
</html>